# Guia de OtimizaÃ§Ã£o de Performance

## ðŸ“Š Melhorias Implementadas

### 1. OtimizaÃ§Ãµes no CÃ³digo Spark

#### âœ… Reparticionamento Inteligente
```python
# ANTES: Cache sem reparticionamento
df1_blocked = self.create_blocking_key(df1).cache()

# DEPOIS: Reparticionamento por blocking_key
df1_blocked = self.create_blocking_key(df1) \
    .repartition(200, "blocking_key") \
    .cache()
```
**Ganho**: Melhor distribuiÃ§Ã£o de dados e paralelismo nos joins.

#### âœ… MaterializaÃ§Ã£o ForÃ§ada
```python
# ForÃ§a materializaÃ§Ã£o do cache antes do join
df1_blocked.count()
df2_blocked.count()
```
**Ganho**: Evita recomputaÃ§Ã£o durante o join.

#### âœ… ConsolidaÃ§Ã£o de PartiÃ§Ãµes na SaÃ­da
```python
# Reduz partiÃ§Ãµes para escrita eficiente
result = matches.select(...).repartition(10)
```
**Ganho**: Menos arquivos pequenos, I/O mais eficiente.

#### âœ… RemoÃ§Ã£o de Count() DesnecessÃ¡rios
```python
# EVITAR em produÃ§Ã£o - operaÃ§Ã£o cara
# logger.info(f"Dataset 1 count: {df1.count()}")

# PREFERIR - apenas para debug
# Use explain() ou logs do Spark UI
```
**Ganho**: Elimina varreduras completas dos dados.

### 2. ConfiguraÃ§Ãµes Spark Otimizadas

#### Adaptive Query Execution (AQE)
```bash
--conf spark.sql.adaptive.enabled=true
--conf spark.sql.adaptive.coalescePartitions.enabled=true
--conf spark.sql.adaptive.skewJoin.enabled=true
--conf spark.sql.adaptive.autoBroadcastJoinThreshold=10MB
```
**BenefÃ­cios**:
- Ajuste dinÃ¢mico de partiÃ§Ãµes
- DetecÃ§Ã£o automÃ¡tica de skew
- Broadcast automÃ¡tico de tabelas pequenas

#### ConfiguraÃ§Ãµes de MemÃ³ria
```bash
--driver-memory 2g
--executor-memory 3g
--conf spark.memory.fraction=0.8          # 80% para execuÃ§Ã£o e storage
--conf spark.memory.storageFraction=0.3   # 30% da fraÃ§Ã£o para cache
```
**BenefÃ­cios**:
- Mais memÃ³ria para computaÃ§Ã£o
- Melhor aproveitamento do cache

#### Paralelismo
```bash
--conf spark.sql.shuffle.partitions=200
--conf spark.default.parallelism=200
```
**Regra**: 2-4x o nÃºmero de cores disponÃ­veis (4 cores Ã— 50 = 200).

#### SerializaÃ§Ã£o
```bash
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer
```
**Ganho**: 2-10x mais rÃ¡pido que Java serialization.

#### CompressÃ£o Columnar
```bash
--conf spark.sql.inMemoryColumnarStorage.compressed=true
--conf spark.sql.inMemoryColumnarStorage.batchSize=10000
```
**Ganho**: Menos memÃ³ria, melhor cache.

#### Speculation
```bash
--conf spark.speculation=true
--conf spark.speculation.multiplier=2
--conf spark.speculation.quantile=0.75
```
**Ganho**: Re-executa tasks lentas em outros executores.

### 3. Melhorias na Infraestrutura

#### Docker Compose - Aumentar Recursos
```yaml
spark-worker-1:
  environment:
    - SPARK_WORKER_MEMORY=6g    # ANTES: 4g
    - SPARK_WORKER_CORES=4      # ANTES: 2
```

#### Adicionar Mais Workers
```yaml
spark-worker-3:
  build:
    context: .
    dockerfile: docker/Dockerfile.spark
  container_name: spark-worker-3
  command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  environment:
    - SPARK_WORKER_MEMORY=4g
    - SPARK_WORKER_CORES=2
```

### 4. OtimizaÃ§Ãµes de Algoritmo

#### Melhorar Blocking Strategy
```python
# ATUAL: Primeiras 3 letras + ano
blocking_key = concat_ws("_", 
                col("nome_completo").substr(1, 3),
                col("data_nascimento").substr(1, 4))

# MELHOR: Soundex + ano + mÃªs
from pyspark.sql.functions import soundex, substring

blocking_key = concat_ws("_",
                soundex(col("nome_completo")),
                substring(col("data_nascimento"), 1, 7))  # YYYY-MM
```

#### PrÃ©-filtrar Pares Ã“bvios
```python
# Eliminar pares onde a diferenÃ§a de idade > 5 anos
from pyspark.sql.functions import abs, year

joined = joined.filter(
    abs(year(col("data_nascimento_1")) - year(col("data_nascimento_2"))) <= 5
)
```

#### Usar MinHash LSH para Similaridade
```python
from pyspark.ml.feature import MinHashLSH, NGram, HashingTF

# Criar assinaturas MinHash para comparaÃ§Ã£o rÃ¡pida
mh = MinHashLSH(inputCol="features", outputCol="hashes", numHashTables=3)
model = mh.fit(dataset)
similar_pairs = model.approxSimilarityJoin(df1, df2, threshold=0.3)
```

### 5. Particionamento de Dados

#### Particionar Datasets por Blocking Key
```python
# Ao salvar dados
df.write \
  .partitionBy("blocking_key_prefix") \
  .parquet(output_path)

# Leitura se torna mais eficiente
df = spark.read.parquet(path).filter(col("blocking_key_prefix") == "A_1990")
```

### 6. Benchmark e Monitoramento

#### Spark UI
```bash
# Acesse durante execuÃ§Ã£o
http://localhost:4040
```
**MÃ©tricas importantes**:
- Stage duration
- Task skew
- Shuffle read/write
- GC time

#### Comandos de Benchmark
```bash
# Tempo de execuÃ§Ã£o
time ./run_matching.sh

# Uso de CPU
docker stats

# Logs detalhados
docker-compose logs -f spark-master | grep "INFO"
```

## ðŸ“ˆ Estimativas de Ganho

| OtimizaÃ§Ã£o | Ganho Estimado | Dificuldade |
|------------|----------------|-------------|
| Reparticionamento | 20-30% | Baixa |
| AQE habilitado | 15-25% | Baixa |
| Remover count() | 10-20% | Baixa |
| Kryo Serializer | 5-10% | Baixa |
| Mais workers | 50-100% | MÃ©dia |
| MemÃ³ria aumentada | 15-30% | Baixa |
| MinHash LSH | 200-500% | Alta |
| Blocking melhorado | 30-50% | MÃ©dia |

## ðŸš€ Roadmap de OtimizaÃ§Ã£o

### Curto Prazo (Implementado)
- âœ… ConfiguraÃ§Ãµes Spark otimizadas
- âœ… Reparticionamento inteligente
- âœ… RemoÃ§Ã£o de count() desnecessÃ¡rios
- âœ… Speculation habilitado

### MÃ©dio Prazo
- [ ] Aumentar recursos Docker (8GB RAM, 4 cores por worker)
- [ ] Adicionar 3Âº worker
- [ ] Implementar blocking key melhorado (Soundex)
- [ ] PrÃ©-filtrar por diferenÃ§a de idade

### Longo Prazo
- [ ] Implementar MinHash LSH
- [ ] Particionar dados por blocking_key
- [ ] Migrar para Apache Flink
- [ ] Usar GPU para computaÃ§Ã£o de similaridade (RAPIDS)

## ðŸ” Debugging de Performance

### Task muito lenta?
```python
# Verificar skew de dados
df.groupBy("blocking_key").count().orderBy(col("count").desc()).show(20)
```

### Muito shuffle?
```python
# Reduzir necessidade de shuffle
df = df.repartition(col("blocking_key"))  # Antes do join
```

### Out of Memory?
```python
# Aumentar partiÃ§Ãµes
spark.conf.set("spark.sql.shuffle.partitions", "400")

# Ou reduzir batch size
spark.conf.set("spark.sql.inMemoryColumnarStorage.batchSize", "5000")
```

### GC overhead?
```bash
# Aumentar memÃ³ria
--executor-memory 6g
--conf spark.memory.fraction=0.9
```

## ðŸ“š ReferÃªncias

- [Spark Performance Tuning](https://spark.apache.org/docs/latest/tuning.html)
- [Adaptive Query Execution](https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution)
- [LSH for Similarity Search](https://spark.apache.org/docs/latest/ml-features.html#lsh-algorithms)
- [Record Linkage Best Practices](https://recordlinkage.readthedocs.io/)
