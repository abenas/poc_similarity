version: '3.8'

services:
  # Apache Spark Master
  spark-master:
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8080:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master Port
    volumes:
      - ./app:/app
      - ./data:/data
    networks:
      - matching-network

  # Spark Worker 1
  spark-worker-1:
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    container_name: spark-worker-1
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=6g
      - SPARK_WORKER_CORES=4
    volumes:
      - ./app:/app
      - ./data:/data
    depends_on:
      - spark-master
    networks:
      - matching-network

  # Spark Worker 2
  spark-worker-2:
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    container_name: spark-worker-2
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=6g
      - SPARK_WORKER_CORES=4
    volumes:
      - ./app:/app
      - ./data:/data
    depends_on:
      - spark-master
    networks:
      - matching-network

  # Spark Worker 3
  spark-worker-3:
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    container_name: spark-worker-3
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=6g
      - SPARK_WORKER_CORES=4
    volumes:
      - ./app:/app
      - ./data:/data
    depends_on:
      - spark-master
    networks:
      - matching-network

  spark-worker-4:
    build:
      context: .
      dockerfile: docker/Dockerfile.spark
    container_name: spark-worker-4
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=6g
      - SPARK_WORKER_CORES=4
    volumes:
      - ./app:/app
      - ./data:/data
    depends_on:
      - spark-master
    networks:
      - matching-network

  # OpenSearch (replacement for Elasticsearch)
  opensearch:
    image: opensearchproject/opensearch:2.11.0
    container_name: opensearch
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms2g -Xmx2g"
      - DISABLE_SECURITY_PLUGIN=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data:/usr/share/opensearch/data
    ports:
      - "9200:9200"  # REST API
      - "9600:9600"  # Performance Analyzer
    networks:
      - matching-network

  # OpenSearch Dashboards
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.11.0
    container_name: opensearch-dashboards
    ports:
      - "5601:5601"
    environment:
      - OPENSEARCH_HOSTS=["http://opensearch:9200"]
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
    depends_on:
      - opensearch
    networks:
      - matching-network

  # Redis for caching and deduplication
  redis:
    image: redis:7.2-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    networks:
      - matching-network

  # LocalStack for AWS services simulation (S3, Glue, DynamoDB)
  localstack:
    image: localstack/localstack:3.0
    container_name: localstack
    ports:
      - "4566:4566"  # LocalStack Gateway
      - "4510-4559:4510-4559"  # External services port range
    environment:
      - SERVICES=s3,dynamodb
      - DEBUG=1
      - PERSISTENCE=1
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - "./localstack-init:/etc/localstack/init/ready.d"
      - localstack-data:/var/lib/localstack
      - "/var/run/docker.sock:/var/run/docker.sock"
      - "./data:/data"
    networks:
      - matching-network

  # Jupyter Notebook for development and testing
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./app:/home/jovyan/work
      - ./notebooks:/home/jovyan/notebooks
      - ./data:/home/jovyan/data
    depends_on:
      - spark-master
    networks:
      - matching-network

  # Application container for running jobs
  app:
    build:
      context: .
      dockerfile: docker/Dockerfile.app
    container_name: person-matcher-app
    volumes:
      - ./app:/app
      - ./data:/data
      - ~/.aws:/root/.aws:ro  # Mount AWS credentials for local testing
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - OPENSEARCH_HOST=opensearch
      - REDIS_HOST=redis
      - AWS_ENDPOINT_URL=http://localstack:4566
    depends_on:
      - spark-master
      - opensearch
      - redis
      - localstack
    networks:
      - matching-network
    command: tail -f /dev/null  # Keep container running

networks:
  matching-network:
    driver: bridge

volumes:
  opensearch-data:
  redis-data:
  localstack-data:
