# üîß Troubleshooting Guide

## Problemas Comuns e Solu√ß√µes

### üê≥ Docker / Desenvolvimento Local

#### Container n√£o inicia

**Sintoma**: `docker-compose up` falha

**Solu√ß√µes**:
```bash
# 1. Verificar portas em uso
sudo lsof -i :8080  # Spark
sudo lsof -i :9200  # OpenSearch
sudo lsof -i :6379  # Redis

# 2. Limpar containers antigos
docker-compose down -v
docker system prune -a

# 3. Reconstruir imagens
docker-compose build --no-cache
docker-compose up -d
```

#### OpenSearch n√£o fica green

**Sintoma**: Cluster status yellow/red

**Solu√ß√µes**:
```bash
# Verificar status
curl http://localhost:9200/_cluster/health?pretty

# Aumentar mem√≥ria
# Editar docker-compose.yml
OPENSEARCH_JAVA_OPTS: "-Xms4g -Xmx4g"

# Reiniciar
docker-compose restart opensearch
```

#### Spark worker desconecta

**Sintoma**: Workers n√£o aparecem no UI

**Solu√ß√µes**:
```bash
# Ver logs
docker-compose logs spark-worker-1

# Aumentar mem√≥ria worker
# Editar docker-compose.yml
SPARK_WORKER_MEMORY: "8G"

# Restart workers
docker-compose restart spark-worker-1 spark-worker-2
```

---

### ‚òÅÔ∏è AWS / Terraform

#### Terraform apply falha

**Sintoma**: Erro ao criar recursos

**Solu√ß√µes**:
```bash
# 1. Verificar credenciais AWS
aws sts get-caller-identity

# 2. Verificar quotas
aws service-quotas list-service-quotas \
  --service-code elasticmapreduce

# 3. Validar terraform
cd terraform
terraform validate
terraform fmt -check

# 4. Reiniciar terraform
rm -rf .terraform .terraform.lock.hcl
terraform init
terraform plan
```

**Erros comuns**:

```
Error: InvalidInstanceType
Solu√ß√£o: Escolha instance type dispon√≠vel na regi√£o
terraform plan -var="emr_core_instance_type=m5.xlarge"

Error: InsufficientCapacity
Solu√ß√£o: Tente outra AZ ou use Spot instances

Error: VPC limit exceeded
Solu√ß√£o: Delete VPCs n√£o utilizadas ou solicite aumento de quota
```

#### EMR Cluster n√£o inicia

**Sintoma**: Cluster fica em STARTING por muito tempo

**Solu√ß√µes**:
```bash
# 1. Verificar logs bootstrap
aws emr describe-cluster --cluster-id j-XXXXX

# 2. Verificar security groups
aws ec2 describe-security-groups --group-ids sg-XXXXX

# 3. Verificar subnet
# Certifique-se que subnet privada tem NAT gateway

# 4. Verificar service role
aws iam get-role --role-name person-matching-emr-service-role
```

#### OpenSearch cluster red

**Sintoma**: Cluster status RED

**Solu√ß√µes**:
```bash
# 1. Verificar shards
curl https://<endpoint>/_cat/shards?v | grep UNASSIGNED

# 2. Realocar shards
curl -X POST "https://<endpoint>/_cluster/reroute?retry_failed=true"

# 3. Aumentar nodes
# Editar terraform/opensearch.tf
opensearch_instance_count = 5
terraform apply

# 4. Verificar disk space
curl https://<endpoint>/_cat/allocation?v

# 5. Se cr√≠tico: restore snapshot
aws opensearch describe-domain --domain-name person-matching-dev
```

#### Lambda timeout

**Sintoma**: Lambda execution timeout

**Solu√ß√µes**:
```bash
# 1. Aumentar timeout
aws lambda update-function-configuration \
  --function-name person-matching-delta-detector \
  --timeout 900

# 2. Aumentar mem√≥ria
aws lambda update-function-configuration \
  --function-name person-matching-delta-detector \
  --memory-size 1024

# 3. Verificar VPC NAT gateway
# Lambda em VPC precisa NAT para acessar AWS services

# 4. Ver logs detalhados
aws logs tail /aws/lambda/person-matching-delta-detector \
  --since 1h --format short
```

---

### üî• Spark / Processamento

#### Job Spark falha com OOM

**Sintoma**: `java.lang.OutOfMemoryError`

**Solu√ß√µes**:
```python
# 1. Aumentar executor memory
spark.conf.set("spark.executor.memory", "20g")
spark.conf.set("spark.executor.memoryOverhead", "4g")

# 2. Aumentar parti√ß√µes
spark.conf.set("spark.sql.shuffle.partitions", "400")

# 3. Repartition dataframe
df = df.repartition(200, "blocking_key")

# 4. Cache com cuidado
# S√≥ cache o que for reutilizado
df.cache()
# ... usar df v√°rias vezes ...
df.unpersist()

# 5. Usar persist em disco se necess√°rio
df.persist(StorageLevel.MEMORY_AND_DISK)
```

#### Job muito lento

**Sintoma**: Processing time > esperado

**Diagn√≥stico**:
```bash
# 1. Acessar Spark UI
http://<emr-master-dns>:18080

# 2. Verificar:
- Skew de dados (tarefas desbalanceadas)
- Shuffle excessivo
- GC time alto
- Stages falhando
```

**Otimiza√ß√µes**:
```python
# 1. Broadcast joins para tabelas pequenas
df_small_broadcast = broadcast(df_small)
result = df_large.join(df_small_broadcast, "key")

# 2. Filtrar cedo
df = df.filter(col("status") == "Ativo")  # Antes de joins

# 3. Selecionar apenas colunas necess√°rias
df = df.select("nome", "data_nascimento", "nr_documento")

# 4. Usar AQE
spark.conf.set("spark.sql.adaptive.enabled", "true")

# 5. Salting para data skew
df = df.withColumn("salt", (rand() * 10).cast("int"))
df = df.repartition("blocking_key", "salt")
```

#### Blocking key com skew

**Sintoma**: Algumas parti√ß√µes muito maiores

**Solu√ß√µes**:
```python
# 1. Adicionar salt ao blocking key
df = df.withColumn(
    "blocking_key_salted",
    concat(col("blocking_key"), lit("_"), (rand() * 5).cast("int"))
)

# 2. Multi-level blocking
# Primeiro blocking: year
# Segundo blocking: first 3 chars
df = df.withColumn("blocking_key_1", year(col("data_nascimento")))
df = df.withColumn("blocking_key_2", substring(col("nome"), 1, 3))

# 3. Adaptive partitioning
df.repartition(200).write.option("maxRecordsPerFile", 100000).parquet(path)
```

---

### üîç OpenSearch

#### Indexa√ß√£o lenta

**Sintoma**: Bulk indexing timeout

**Solu√ß√µes**:
```python
# 1. Aumentar refresh interval durante bulk
PUT /person-matches/_settings
{
  "index": {
    "refresh_interval": "30s",
    "number_of_replicas": 0
  }
}

# Ap√≥s indexa√ß√£o:
PUT /person-matches/_settings
{
  "index": {
    "refresh_interval": "1s",
    "number_of_replicas": 1
  }
}

# 2. Usar bulk API com batch size otimizado
batch_size = 1000  # Testar entre 500-2000

# 3. Aumentar thread pool
PUT /_cluster/settings
{
  "transient": {
    "threadpool.write.queue_size": 1000
  }
}
```

#### Busca lenta

**Sintoma**: Query time > 1s

**Solu√ß√µes**:
```bash
# 1. Analisar slow logs
GET /person-matches/_settings/index.search.slowlog*

# 2. Usar explain API
GET /person-matches/_search
{
  "explain": true,
  "query": {...}
}

# 3. Otimizar query
# Usar term queries para campos keyword
{
  "query": {
    "term": {"nr_documento_1": "12345"}  # Mais r√°pido que match
  }
}

# 4. Force merge para reduzir segments
POST /person-matches/_forcemerge?max_num_segments=1
```

---

### üíæ S3 / Glue

#### Crawler n√£o encontra dados

**Sintoma**: Glue table vazia

**Solu√ß√µes**:
```bash
# 1. Verificar path S3
aws s3 ls s3://person-matching-data-source1-dev/

# 2. Verificar permiss√µes IAM
aws iam get-role-policy \
  --role-name person-matching-glue-crawler-role \
  --policy-name person-matching-glue-s3-policy

# 3. Verificar formato arquivo
# Glue detecta: Parquet, JSON, CSV, Avro
# Arquivo deve ter extens√£o correta (.parquet)

# 4. Re-run crawler
aws glue start-crawler --name person-matching-dataset1-crawler
aws glue get-crawler --name person-matching-dataset1-crawler
```

#### Schema evolution

**Sintoma**: Colunas novas n√£o aparecem

**Solu√ß√µes**:
```bash
# 1. Configurar crawler para UPDATE_IN_DATABASE
aws glue update-crawler --name person-matching-dataset1-crawler \
  --schema-change-policy '{
    "UpdateBehavior": "UPDATE_IN_DATABASE",
    "DeleteBehavior": "LOG"
  }'

# 2. Re-run crawler
aws glue start-crawler --name person-matching-dataset1-crawler

# 3. Ou atualizar tabela manualmente
aws glue update-table --database-name person_data \
  --table-input file://new_schema.json
```

---

### üìä Performance Issues

#### Alto custo

**Sintoma**: Bill AWS maior que esperado

**An√°lise**:
```bash
# 1. Cost Explorer
aws ce get-cost-and-usage \
  --time-period Start=2024-11-01,End=2024-11-30 \
  --granularity DAILY \
  --metrics BlendedCost \
  --group-by Type=SERVICE

# 2. Verificar recursos ociosos
aws emr list-clusters --active
aws opensearch list-domain-names
aws elasticache describe-replication-groups
```

**Otimiza√ß√µes de custo**:
```bash
# 1. Usar Spot instances EMR (70% economia)
# Editar terraform/emr.tf
market = "SPOT"
bid_price = "0.30"

# 2. Auto-scaling agressivo
emr_min_capacity = 1
emr_max_capacity = 5

# 3. Schedule start/stop EMR
# Lambda para stop cluster quando n√£o usado

# 4. S3 Intelligent-Tiering
aws s3api put-bucket-intelligent-tiering-configuration \
  --bucket person-matching-results-dev \
  --id config1 \
  --intelligent-tiering-configuration file://tiering.json

# 5. OpenSearch Reserved Instances
# 1-year: 30% desconto
# 3-years: 50% desconto
```

---

### üö® Erros Comuns

#### "Access Denied" S3

```bash
# Verificar bucket policy
aws s3api get-bucket-policy --bucket person-matching-data-source1-dev

# Verificar IAM role
aws iam get-role-policy --role-name person-matching-emr-ec2-role \
  --policy-name person-matching-emr-ec2-policy

# Testar acesso
aws s3 ls s3://person-matching-data-source1-dev/ \
  --profile emr-role
```

#### "ResourceNotFoundException" DynamoDB

```bash
# Verificar tabela existe
aws dynamodb describe-table --table-name person-matching-state

# Verificar regi√£o correta
aws dynamodb list-tables --region us-east-1

# Recriar tabela
cd terraform
terraform taint aws_dynamodb_table.state
terraform apply
```

#### "ClusterNotFoundException" EMR

```bash
# Listar clusters
aws emr list-clusters --active

# Se n√£o existe, criar
cd terraform
terraform apply

# Se existe mas n√£o aparece, verificar regi√£o
aws emr list-clusters --region us-east-1
```

---

### üìû Suporte

#### Logs para debug

```bash
# Coletar todos logs relevantes
mkdir debug-logs
cd debug-logs

# Terraform
terraform show > terraform-state.txt

# EMR
aws emr describe-cluster --cluster-id j-XXX > emr-cluster.json
aws logs get-log-events --log-group-name /aws/emr/XXX > emr-logs.txt

# Lambda
aws logs tail /aws/lambda/person-matching-delta-detector > lambda-logs.txt

# OpenSearch
curl https://<endpoint>/_cluster/health > opensearch-health.json
curl https://<endpoint>/_cat/indices?v > opensearch-indices.txt

# Zipar e enviar
zip -r debug-logs.zip .
```

#### Checklist debug

```
‚ñ° AWS credentials v√°lidas
‚ñ° Regi√£o correta configurada
‚ñ° VPC e subnets corretas
‚ñ° Security groups permitem tr√°fego
‚ñ° IAM roles t√™m permiss√µes necess√°rias
‚ñ° S3 buckets existem e t√™m dados
‚ñ° Glue tables criadas e populadas
‚ñ° EMR cluster RUNNING
‚ñ° OpenSearch cluster GREEN
‚ñ° Lambda sem erros
‚ñ° CloudWatch logs sem errors
‚ñ° Billing alerts configurados
```

---

**√öltima atualiza√ß√£o**: 2024-11-17  
**Vers√£o**: 1.0
